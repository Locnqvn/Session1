{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "def text_data(pathdata):\n",
    "    # read data\n",
    "    with open(pathdata,'r') as f:\n",
    "        text=f.read()\n",
    "    text=text.lower()  #Convert text to lowercase\n",
    "    text=re.sub(r'\\d+',' ',text) # Remove numbers\n",
    "    text=re.sub(r'_',' ',text)  # Remove '_' character\n",
    "    words=re.split(r'\\W+',text) # Remove punctuation and Tokenization\n",
    "    words=[ PorterStemmer().stem(word)\n",
    "            for word in words\n",
    "            if word not in stopwords.words('english') ]  # Remove stopwords\n",
    "    text=' '.join(words)\n",
    "    return text\n",
    "\n",
    "def collect_data(pathin):\n",
    "    data=[]\n",
    "    label=0\n",
    "    listfolder=os.listdir(pathin) # load folders name\n",
    "    for folder in listfolder:\n",
    "        pathdocument=pathin+\"/\"+folder\n",
    "        listdocument=os.listdir(pathdocument)\n",
    "        for doct in listdocument:\n",
    "            text=text_data(pathdocument+\"/\"+doct)\n",
    "            data.append(str(label)+\"<<>>\"+doct+\"<<>>\"+text)\n",
    "        label+=1\n",
    "    text='\\n'.join(data)\n",
    "    return text\n",
    "\n",
    "text_train=collect_data(\"/home/lnq/Desktop/20192/code_lab/Text Preprocessing/20news-bydate-train\")\n",
    "text_test=collect_data(\"/home/lnq/Desktop/20192/code_lab/Text Preprocessing/20news-bydate-test\")\n",
    "with open(\"/home/lnq/Desktop/20192/code_lab/Text Preprocessing/20news-bydate-test/text_test\",'w') as f:\n",
    "    f.write(text_test)\n",
    "with open(\"/home/lnq/Desktop/20192/code_lab/Text Preprocessing/20news-bydate-traintext_train\",'w') as f:\n",
    "    f.write(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature vector\n",
    "import numpy as np\n",
    "\n",
    "# compute idf function\n",
    "def compute_idf(numducument, df):\n",
    "    assert df > 0\n",
    "    return np.log10(numducument/df*1.)\n",
    "\n",
    "# compute tf-idf function\n",
    "def compute_tfidf(fwd,tfmax,idf):\n",
    "    return idf*1.0*fwd/tfmax\n",
    "# find index word in list words with binary search\n",
    "def isappear(word,wordslist):\n",
    "    l=0\n",
    "    r=len(wordslist)-1\n",
    "    while(l<=r):\n",
    "        m=int((r+l)/2)\n",
    "        if(word==wordslist[m]):\n",
    "            return m\n",
    "        else:\n",
    "            if(word<wordslist[m]):\n",
    "                r=m-1\n",
    "            else:\n",
    "                l=m+1\n",
    "    return -1\n",
    "\n",
    "# compute feature and idf\n",
    "def word_feature(pathin):\n",
    "    words_list = []\n",
    "    cntdocument = 0\n",
    "    with open(pathin, 'r') as f:\n",
    "        data = f.read().splitlines()\n",
    "    #find words list\n",
    "    for document in data:\n",
    "        text_data = document.split(\"<<>>\")\n",
    "        text = text_data[-1].split()\n",
    "        words_list += list(set(text))\n",
    "    words_list = list(set(words_list))\n",
    "    words_list.sort()\n",
    "    words_df = dict.fromkeys(words_list, 0)\n",
    "    # compute df\n",
    "    for document in data:\n",
    "        text_data = document.split(\"<<>>\")\n",
    "        text = set(text_data[-1].split())\n",
    "        for word in text:\n",
    "            words_df[word] += 1\n",
    "        cntdocument += 1\n",
    "    # compute words_idf\n",
    "    words_idf = { word : compute_idf(cntdocument, words_df[word])\n",
    "                 for word in list(words_df.keys())\n",
    "                 if words_df[word] > 3}\n",
    "    return words_idf\n",
    "\n",
    "\n",
    "# write words list and idf function\n",
    "def write_idf(words_idf):\n",
    "    n=len(words_idf)\n",
    "    words=list(words_idf.keys())\n",
    "    idf=list(words_idf.values())\n",
    "    with open(\"/home/lnq/Desktop/20192/code_lab/Text Preprocessing/words_list\",'w') as f:\n",
    "        f.write(' '.join(words))\n",
    "    with open(\"/home/lnq/Desktop/20192/code_lab/Text Preprocessing/words_idf\", 'w') as f:\n",
    "        for i in range(n):\n",
    "            f.write(words[i]+\"<<>>\"+str(idf[i])+\"\\n\")\n",
    "\n",
    "\n",
    "def find_feature_vector(pathdct):\n",
    "    words_idf=word_feature(pathdct) # find words list and compute idf for each word in words list\n",
    "    # write idf\n",
    "    write_idf(words_idf)\n",
    "    feature_words=list(words_idf.keys())\n",
    "    idf=list(words_idf.values())\n",
    "    n=len(words_idf)\n",
    "    feature_matrix=[]\n",
    "    with open(pathdct,'r') as f:\n",
    "        data=f.read().splitlines()  # load data\n",
    "    #compute tf-idf for each document\n",
    "    for dct in data:\n",
    "        ifo=dct.split(\"<<>>\")\n",
    "        listwords=ifo[2].split()\n",
    "        setwords=set(listwords)\n",
    "        # find the words in document that is appear in words list  \n",
    "        words=[ word for word in setwords\n",
    "                if (isappear(word,feature_words)!=-1)]\n",
    "        tfmax=max([ listwords.count(word) for word in words ])\n",
    "        dct_vector=ifo[0]+\"<<>>\"+ifo[1]+\"<<>>\"\n",
    "        # compute tf-idf for each word\n",
    "        for word in words:\n",
    "            pos=isappear(word,feature_words)\n",
    "            dct_vector+=str(pos)+\":\"+str(compute_tfidf(listwords.count(word),tfmax,idf[pos]))+\" \"\n",
    "        feature_matrix.append(dct_vector)\n",
    "    return feature_matrix\n",
    "\n",
    "tf_idf=find_feature_vector(\"/home/lnq/Desktop/20192/code_lab/Text Preprocessing/text_train\")\n",
    "# write feature matrix\n",
    "with open(\"/home/lnq/Desktop/20192/code_lab/Text Preprocessing/train_tfidf_df=3\",'w') as f:\n",
    "    f.write('\\n'.join(tf_idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38232bit5cde167e725747d38e0f0bb812fb51e0",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}